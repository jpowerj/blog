[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jeff Jacobs",
    "section": "",
    "text": "Hi, I‚Äôm Jeff Jacobs! I am an Assistant Teaching Professor of Data Science and Analytics at Georgetown University.\nI post things here when my brain becomes obsessed with something and needs to get to the bottom of it (since otherwise I annoy friends and family with these things for days on end üòú).\nIf you are one of my students, however, you may be looking for my DSAN Resources page, where I post things that are relevant to Georgetown DSAN students. Otherwise, you can find more on my academic homepage!"
  },
  {
    "objectID": "posts/2024-05-06-a-meaningful-passage/index.html",
    "href": "posts/2024-05-06-a-meaningful-passage/index.html",
    "title": "A Meaningful Passage",
    "section": "",
    "text": "(My sloppy attempt at translation!)\n\n\nKamaswami told him about his business, he showed him wares and warehouses, showed him accounts. Siddhartha learned. He heard much and said little. He treated it all as a game, whose rules he strove to learn precisely, but whose content did not touch his heart.\nIn matters of love he was childish. He tended to pursue blindly, endlessly, insatiably. She taught him to balance taking with giving, and that every gesture, every caress, every touch, every glance, every aspect of a connection has its secret to patiently unlock. She taught him that in each celebration of love the lovers should admire each other, without being conquered or having conquered, so that neither is glutted, neither has the feeling of having misused or having been misused. He spent wonderful hours with the clever and beautiful artist, became her pupil, her lover, her friend. Here, with Kamala, lay the value and purpose of his current life, and not with Kamaswami‚Äôs business. \n\n(Hesse 1922)\n\n\n\n\n\n\nHesse, Hermann. 1922. Siddhartha. New Directions."
  },
  {
    "objectID": "posts/2023-12-14-nfl-parity/index.html",
    "href": "posts/2023-12-14-nfl-parity/index.html",
    "title": "The NFL Has Become (Slightly) More Boring Over Time",
    "section": "",
    "text": "As a theme that will reappear across several of these posts, we‚Äôll see that when we try to translate the interpretive idea of ‚Äúinterestingness‚Äù into a measurable quantity, entropy will be precisely the tool we‚Äôll want to use! As a reminder, information entropy is just a measure of how predictable the outcome of a stochastic process is:\n\n\n\n\n\nIn the above figure, for example, each point in the plot represents a distribution, which we can think of like the contents of a ‚Äúbag‚Äù that we are going to reach into and pull an object out of:\n\nThe ‚Äúbag‚Äù in the middle with equally many plus signs and minus signs has the highest entropy over all possible ‚Äúbags‚Äù, since we cannot predict better than 50/50 what we will pull out when we reach into the bag.\nThe bag all the way on the left, on the other hand, has the lowest possible entropy, since when we reach into this bag we know with 100% certainty that we will pull out a minus sign (and similarly for the bag all the way on the right, where we know with 100% certainty that we will pull out a plus sign).\n\nIn one of my favorite sports analyses of all time, for example, Jon Bois demonstrates the usefulness of entropy by going through every NFL team‚Äôs historcal record, whereby we can see that consistent teams (whether consistently good or consistently bad) are exactly those teams with the lowest entropy, while the most volatile teams, manically oscillating between dominant and pathetic seasons, have the highest entropy:\n\nIn this analysis I‚Äôm interested in a similar phenomenon, but from the perspective of someone like me: my home team has been so consistently, spectacularly bad for my entire life that I‚Äôve had to just enjoy watching the NFL as a whole, rather than following that one specific team. Because of this, to me, ‚Äúexciting‚Äù seasons are ones in which any team could potentially beat any other team on a given day, whereas ‚Äúboring‚Äù seasons are those in which the usual dynasties (in the 90s: Packers, Broncos; in the 2000s: Patriots, Colts) dominate all others.\n\nSo, to quell my curiosity, I used the same dataset but ranked each season in terms of unpredictability. That is, in terms of how well we can predict the winner by knowing which team has a better record. This way of looking at it is captured perfectly by the phrase often used by announcers after astonishing upsets: ‚ÄúThat‚Äôs why they play the game!‚Äù"
  },
  {
    "objectID": "posts/2023-12-14-nfl-parity/index.html#what-makes-an-nfl-season-interesting",
    "href": "posts/2023-12-14-nfl-parity/index.html#what-makes-an-nfl-season-interesting",
    "title": "The NFL Has Become (Slightly) More Boring Over Time",
    "section": "",
    "text": "As a theme that will reappear across several of these posts, we‚Äôll see that when we try to translate the interpretive idea of ‚Äúinterestingness‚Äù into a measurable quantity, entropy will be precisely the tool we‚Äôll want to use! As a reminder, information entropy is just a measure of how predictable the outcome of a stochastic process is:\n\n\n\n\n\nIn the above figure, for example, each point in the plot represents a distribution, which we can think of like the contents of a ‚Äúbag‚Äù that we are going to reach into and pull an object out of:\n\nThe ‚Äúbag‚Äù in the middle with equally many plus signs and minus signs has the highest entropy over all possible ‚Äúbags‚Äù, since we cannot predict better than 50/50 what we will pull out when we reach into the bag.\nThe bag all the way on the left, on the other hand, has the lowest possible entropy, since when we reach into this bag we know with 100% certainty that we will pull out a minus sign (and similarly for the bag all the way on the right, where we know with 100% certainty that we will pull out a plus sign).\n\nIn one of my favorite sports analyses of all time, for example, Jon Bois demonstrates the usefulness of entropy by going through every NFL team‚Äôs historcal record, whereby we can see that consistent teams (whether consistently good or consistently bad) are exactly those teams with the lowest entropy, while the most volatile teams, manically oscillating between dominant and pathetic seasons, have the highest entropy:\n\nIn this analysis I‚Äôm interested in a similar phenomenon, but from the perspective of someone like me: my home team has been so consistently, spectacularly bad for my entire life that I‚Äôve had to just enjoy watching the NFL as a whole, rather than following that one specific team. Because of this, to me, ‚Äúexciting‚Äù seasons are ones in which any team could potentially beat any other team on a given day, whereas ‚Äúboring‚Äù seasons are those in which the usual dynasties (in the 90s: Packers, Broncos; in the 2000s: Patriots, Colts) dominate all others.\n\nSo, to quell my curiosity, I used the same dataset but ranked each season in terms of unpredictability. That is, in terms of how well we can predict the winner by knowing which team has a better record. This way of looking at it is captured perfectly by the phrase often used by announcers after astonishing upsets: ‚ÄúThat‚Äôs why they play the game!‚Äù"
  },
  {
    "objectID": "posts/2023-12-14-nfl-parity/index.html#data-analysis",
    "href": "posts/2023-12-14-nfl-parity/index.html#data-analysis",
    "title": "The NFL Has Become (Slightly) More Boring Over Time",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\nCode\nimport pandas as pd\nimport numpy as np\ncombined_df = pd.read_csv(\"assets/nfl_combined.csv\")\ncombined_df.head()\n\n\n\n\n\n\n\n\n\n\ngame_id\nseason\ngame_type\nweek\ngameday\nweekday\ngametime\naway_team\naway_score\nhome_team\n...\nWinner/tie\nat\nLoser/tie\nUnnamed: 7\nPtsW\nPtsL\nYdsW\nTOW\nYdsL\nTOL\n\n\n\n\n0\n1999_01_MIN_ATL\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nMIN\n17.0\nATL\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1999_01_KC_CHI\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nKC\n17.0\nCHI\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1999_01_PIT_CLE\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nPIT\n43.0\nCLE\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1999_01_OAK_GB\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nOAK\n24.0\nGB\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1999_01_BUF_IND\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nBUF\n14.0\nIND\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 61 columns\n\n\n\n\nThen, because my brain is forever stuck in an object-oriented mode (and because I‚Äôm teaching Data Structures in Python next semester!), I decided to keep track of each team‚Äôs record throughout each season using custom Season and SeasonTeam objects:\n\n\nCode\n# Get the range of seasons from the df\nfirst_year_full = combined_df['season'].min()\nlast_year_full = combined_df['season'].max()\nprint(first_year_full, last_year_full)\nyear_range_full = list(range(first_year_full, last_year_full + 1))\nclass Season:\n    def __init__(self, year, team_id_list):\n        self.year = year\n        # Keys will be {year}_{team}\n        self.teams = {team_id: SeasonTeam(team_id, self.year) for team_id in team_id_list}\n        \n    def __str__(self):\n        all_teams = self.get_team_list()\n        first_team = all_teams[0]\n        last_team = all_teams[-1]\n        return f\"Season[year={self.get_year()},{self.get_num_teams()} teams: [{first_team}, ..., {last_team}]]\"\n    \n    def __repr__(self):\n        return self.__str__()\n        \n    def add_team(self, team_id):\n        self.teams[team_id] = SeasonTeam(team_id, self.year)\n        \n    def get_num_teams(self):\n        return len(self.get_team_list())\n    \n    def get_team(self, team_id):\n        return self.teams[team_id]\n    \n    def get_team_list(self):\n        return list(self.teams.keys())\n    \n    def get_team_record(self, team_id):\n        return self.get_team(team_id).get_record()\n    \n    def get_year(self):\n        return self.year\n    \n    def record_result(self, team_id, result):\n        self.get_team(team_id).record_result(result)\n\nclass SeasonTeam:\n    def __init__(self, team, year):\n        self.team = team\n        self.year = year\n        # (w,l,t), first week starts at (0,0,0)\n        self.record = np.array([0,0,0])\n    \n    def get_record(self):\n        return self.record\n    \n    def record_result(self, result):\n        new_record = self.get_record() + result\n        self.set_record(new_record)\n        \n    def set_record(self, new_record):\n        self.record = new_record\n\n\n1987 2021\n\n\nSo that now we can use a dictionary of Season objects to keep track of season-by-season data:\n\n\nCode\nunique_home = set(combined_df['away_team'].unique())\nunique_away = set(combined_df['home_team'].unique())\nunique_teams_set = unique_home.union(unique_away)\nteam_ids = sorted(list(unique_teams_set))\nprint(team_ids)\nprint(year_range_full)\nseasons = {cur_year: Season(cur_year, team_ids) for cur_year in year_range_full}\nprint(seasons[1999])\nprint(seasons.keys())\n\n\n['ARI', 'ATL', 'BAL', 'BUF', 'CAR', 'CHI', 'CIN', 'CLE', 'DAL', 'DEN', 'DET', 'GB', 'HOU', 'IND', 'JAX', 'KC', 'LA', 'LAC', 'LAR', 'LV', 'MIA', 'MIN', 'NE', 'NO', 'NYG', 'NYJ', 'OAK', 'PHI', 'PIT', 'SD', 'SEA', 'SF', 'STL', 'TB', 'TEN', 'WAS']\n[1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\nSeason[year=1999,36 teams: [ARI, ..., WAS]]\ndict_keys([1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021])\n\n\nAnd after defining some helper functions:\n\n\nCode\n# Ties count as 0.5 win and 0.5 loss\nwin_vec = np.array([1, 0, 0.5])\nloss_vec = np.array([0, 1, 0.5])\ndef compute_win_pct(record_vec):\n    if np.sum(record_vec) == 0:\n        # No games played yet, win pct considered 0\n        return 0\n    total_wins = np.dot(record_vec, win_vec)\n    total_losses = np.dot(record_vec, loss_vec)\n    win_pct = total_wins / (total_wins + total_losses)\n    return win_pct\n\ndef compare_records(record_a, record_b):\n    pct_a = compute_win_pct(record_a)\n    pct_b = compute_win_pct(record_b)\n    if pct_a &gt; pct_b:\n        return 1\n    if pct_b &gt; pct_a:\n        return -1\n    return 0\n\n\nI processed each game in a giant, completely-inefficient loop, which could definitely be done in a more efficient data-sciency way!\n\n\nCode\nall_result_data = []\nfor row_index, row in combined_df.iterrows():\n    cur_game_id = row['game_id']\n    cur_season = row['season']\n    season_obj = seasons[cur_season]\n    cur_week = row['week']\n    cur_away = row['away_team']\n    cur_home = row['home_team']\n    cur_result = row['result']\n    #print(cur_away, cur_home, cur_result)\n    away_pre_record = season_obj.get_team_record(cur_away)\n    home_pre_record = season_obj.get_team_record(cur_home)\n    away_better = compare_records(away_pre_record, home_pre_record)\n    if away_better &gt; 0:\n        better_team = cur_away\n    elif away_better &lt; 0:\n        better_team = cur_home\n    else:\n        better_team = \"none\"\n    #print(cur_away, away_pre_record, cur_home, home_pre_record, away_better)\n    if cur_result &lt; 0:\n        # Away team won\n        winning_team = cur_away\n        away_result = np.array([1,0,0])\n        home_result = np.array([0,1,0])\n    elif cur_result &gt; 0:\n        # Home team won\n        winning_team = cur_home\n        home_result = np.array([1,0,0])\n        away_result = np.array([0,1,0])\n    else:\n        # Tie\n        winning_team = \"none\"\n        away_result = np.array([0,0,1])\n        home_result = np.array([0,0,1])\n    season_obj.record_result(cur_away, away_result)\n    season_obj.record_result(cur_home, home_result)\n    away_post_record = season_obj.get_team_record(cur_away)\n    home_post_record = season_obj.get_team_record(cur_home)\n    #print(cur_away, away_post_record, cur_home, home_post_record)\n    # Now we can create the results data\n    result_data = {\n        'game_id': cur_game_id,\n        'away_pre': away_pre_record,\n        'home_pre': home_pre_record,\n        'better_team': better_team,\n        'winning_team': winning_team,\n        'better_won': (better_team != \"none\") and (better_team == winning_team),\n        'away_result': away_result,\n        'home_result': home_result,\n        'away_post': away_post_record,\n        'home_post': home_post_record\n    }\n    all_result_data.append(result_data)\n\n\nThe all_result_data list can now immediately be converted into a Pandas DataFrame:\n\n\nCode\nresult_df = pd.DataFrame(all_result_data)\nresult_df.head()\n\n\n\n\n\n\n\n\n\n\ngame_id\naway_pre\nhome_pre\nbetter_team\nwinning_team\nbetter_won\naway_result\nhome_result\naway_post\nhome_post\n\n\n\n\n0\n1999_01_MIN_ATL\n[0, 0, 0]\n[0, 0, 0]\nnone\nMIN\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n\n\n1\n1999_01_KC_CHI\n[0, 0, 0]\n[0, 0, 0]\nnone\nCHI\nFalse\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n\n\n2\n1999_01_PIT_CLE\n[0, 0, 0]\n[0, 0, 0]\nnone\nPIT\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n\n\n3\n1999_01_OAK_GB\n[0, 0, 0]\n[0, 0, 0]\nnone\nGB\nFalse\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n\n\n4\n1999_01_BUF_IND\n[0, 0, 0]\n[0, 0, 0]\nnone\nIND\nFalse\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n\n\n\n\n\n\n\n\nBut this reveals an important consideration, which is that we should specifically focus on only those games where there was a team with an unambiguously-better record:\n\n\nCode\nresult_comp_df = result_df[result_df['better_team'] != \"none\"].copy()\nresult_comp_df.head()\n\n\n\n\n\n\n\n\n\n\ngame_id\naway_pre\nhome_pre\nbetter_team\nwinning_team\nbetter_won\naway_result\nhome_result\naway_post\nhome_post\n\n\n\n\n15\n1999_02_PIT_BAL\n[1, 0, 0]\n[0, 1, 0]\nPIT\nPIT\nTrue\n[1, 0, 0]\n[0, 1, 0]\n[2, 0, 0]\n[0, 2, 0]\n\n\n17\n1999_02_JAX_CAR\n[1, 0, 0]\n[0, 1, 0]\nJAX\nJAX\nTrue\n[1, 0, 0]\n[0, 1, 0]\n[2, 0, 0]\n[0, 2, 0]\n\n\n18\n1999_02_SEA_CHI\n[0, 1, 0]\n[1, 0, 0]\nCHI\nSEA\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 1, 0]\n[1, 1, 0]\n\n\n23\n1999_02_OAK_MIN\n[0, 1, 0]\n[1, 0, 0]\nMIN\nOAK\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 1, 0]\n[1, 1, 0]\n\n\n25\n1999_02_WAS_NYG\n[0, 1, 0]\n[1, 0, 0]\nNYG\nWAS\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 1, 0]\n[1, 1, 0]\n\n\n\n\n\n\n\n\nSo that now we can find the overall proportion of all games where the team with the better record indeed won:\n\n\nCode\nresult_comp_df['better_won'].mean()\n\n\n0.6188321787077619\n\n\nAnd then we can use Pandas‚Äô groupby() to compute this mean for each season:\n\n\nCode\nfrom itables import show\nresult_comp_df['season'] = result_comp_df['game_id'].apply(lambda x: int(x.split(\"_\")[0]))\nmean_by_season = result_comp_df.groupby('season')['better_won'].mean().reset_index()\nshow(mean_by_season)\n\n\n\n\n    \n      \n      season\n      better_won\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/2023-12-14-nfl-parity/index.html#plotting-results",
    "href": "posts/2023-12-14-nfl-parity/index.html#plotting-results",
    "title": "The NFL Has Become (Slightly) More Boring Over Time",
    "section": "Plotting Results",
    "text": "Plotting Results\nFirst, plotting these means as a line graph gives some insight, but is a bit messy due to the season-to-season volatility:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#plt.figure(figsize=(11,8))\nseason_plot = sns.lineplot(x='season', y='better_won', data=mean_by_season, marker='o')\n#plt.xticks(rotation=45, ha='right')\nplt.title(\"Predictability of NFL Games, 1987-2021\")\nplt.xlabel(\"Season\")\nplt.ylabel(\"Pr(Win | Better Record)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can start to see a general trend, though, if we plot a line of best fit, with the important caveats that this is to indicate a general trend, not that we actually think the underlying data-generating process is linear!\n\n\nCode\nseason_regplot = sns.regplot(x='season', y='better_won', data=mean_by_season, ci=89) #, lowess=True)\nseason_regplot.axhline(mean_by_season['better_won'].mean(), linestyle=\"dashed\")\nplt.title(\"Predictability of NFL Games, 1987-2021\")\nplt.xlabel(\"Season\")\nplt.ylabel(\"Pr(Win | Better Record)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd there you have it: the NFL has gotten slightly more boring over time, as measured by the ability to predict game outcomes from the records of the teams going into the game‚Ä¶"
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html",
    "href": "posts/2023-11-27-dc-segregation/index.html",
    "title": "Visualizing Segregation in DC",
    "section": "",
    "text": "About ten years ago, Pew Research released an incredible set of maps visualizing how extreme segregation is in DC, race-wise as well as socioeconomic. A screenshot from this old set of visualizations shows what they used to look like:\nUnfortunately, all of these visualizations used MapBox, which seems to just totally not exist anymore (at least, these particular maps are long gone), so that when you try to view these visualizations on Pew‚Äôs website nowadays, you just get a blank page.\nSo, in this document, I recreate the above maps, using open-source libraries in Python to (hopefully) allow interactive visualization of this important information that will last longer than the previous versions in MapBox‚Äôs proprietary format!"
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html#data-overview",
    "href": "posts/2023-11-27-dc-segregation/index.html#data-overview",
    "title": "Visualizing Segregation in DC",
    "section": "Data Overview",
    "text": "Data Overview\nThe data behind these maps is somewhat hard to find, but in a strange way that is the opposite of most hard-to-find data cases: here there are so many different data sources for income across the ‚ÄúDC Metro Area‚Äù (the definition of this region, itself, being subject to different interpretations by different data sources), that I ran into the following tradeoff at the start:\n\nIf we want data for just the District of Columbia itself, we can obtain very easy-to-use data directly from the DC government‚Äôs data portal, which is ready for immediate use in the sense that we can plug it into a mapping app and see the data without any need to tweak any settings! Clicking that link, for example, will show a preview of the map directly within the GitHub page! While the GitHub preview won‚Äôt show the income data for each tract, this geojson.io link (with the URL just pointing to that GitHub page) will!\nSimilarly, if we want data for just Maryland or just Virginia, we could obtain fairly easy-to-use geoJSON files from these states‚Äô data portals\nBut, if we want data for the DC Metro Area, allowing apples-to-apples comparisons between (for example) census tracts within DC and in the Maryland suburbs, then we run into a bit of an issue since the relevant US Census data is far less ready-for-use in its raw form."
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html#ipums-data-median-income-by-census-tract",
    "href": "posts/2023-11-27-dc-segregation/index.html#ipums-data-median-income-by-census-tract",
    "title": "Visualizing Segregation in DC",
    "section": "IPUMS Data: Median Income by Census Tract",
    "text": "IPUMS Data: Median Income by Census Tract\nFirst we load the data, which contains median household income for all census tracts in the US:\n\nimport pandas as pd\nipums_df = pd.read_csv(\"assets/nhgis0001_ds254_20215_tract.csv\", encoding_errors='ignore')\nipums_df.head()\n\n\n\n\n\n\n\n\n\nGISJOIN\nYEAR\nSTUSAB\nREGIONA\nDIVISIONA\nSTATE\nSTATEA\nCOUNTY\nCOUNTYA\nCOUSUBA\n...\nPCI\nPUMAA\nGEO_ID\nBTTRA\nBTBGA\nTL_GEO_ID\nNAME_E\nAOQIE001\nNAME_M\nAOQIM001\n\n\n\n\n0\nG0100010020100\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020100\nNaN\nNaN\n1001020100\nCensus Tract 201, Autauga County, Alabama\n57399.0\nCensus Tract 201, Autauga County, Alabama\n10706.0\n\n\n1\nG0100010020200\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020200\nNaN\nNaN\n1001020200\nCensus Tract 202, Autauga County, Alabama\n52176.0\nCensus Tract 202, Autauga County, Alabama\n5849.0\n\n\n2\nG0100010020300\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020300\nNaN\nNaN\n1001020300\nCensus Tract 203, Autauga County, Alabama\n63704.0\nCensus Tract 203, Autauga County, Alabama\n11304.0\n\n\n3\nG0100010020400\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020400\nNaN\nNaN\n1001020400\nCensus Tract 204, Autauga County, Alabama\n70000.0\nCensus Tract 204, Autauga County, Alabama\n12155.0\n\n\n4\nG0100010020501\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020501\nNaN\nNaN\n1001020501\nCensus Tract 205.01, Autauga County, Alabama\n60917.0\nCensus Tract 205.01, Autauga County, Alabama\n29232.0\n\n\n\n\n5 rows √ó 45 columns\n\n\n\n\nWe can get a sense of how many Census Tracts there are across different states, before we restrict ourselves to just the DMV:\n\n# Here you can uncomment the following to install itables,\n# if it is not already installed in your environment!\n# We just use this to display nice HTML tables with pagination,\n# so it's optional and you don't need to worry if it\n# fails to install for whatever reason.\n#!pip install itables\n\n\nfrom itables import show\ntract_counts = ipums_df['STUSAB'].value_counts().to_frame().reset_index()\nshow(tract_counts)\n\n\n\n    \n      \n      index\n      STUSAB\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nBut now we can restrict our analysis to just DC, Maryland, and Virginia:\n\nstates_to_include = [\n    'District of Columbia',\n    'Maryland',\n    'Virginia'\n]\ndmv_df = ipums_df[ipums_df['STATE'].isin(states_to_include)].copy()\n\nAnd we can look at the 153 unique values that are listed in the ‚ÄúCounty‚Äù field for these states, where you‚Äôll see that this corresponds not only to ‚Äúcounties‚Äù in the standard colloquial sense but also to areas that have not been incorporated into any counties: places like Alexandria city:\n\ncounty_counts = dmv_df['COUNTY'].value_counts(dropna=False)\nshow(county_counts)\n\n\n\n    \n      \n      COUNTY\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nGoing through these unique values, I select the areas that seemed to be included in Pew‚Äôs ‚ÄúDC Metro Area‚Äù map:\n\ncounties = [\n    'Fairfax County', # 274 tracts\n    'Montgomery County', # 255 tracts\n    \"Prince George's County\", # 214 tracts\n    'District of Columbia', # 206 tracts\n    'Arlington County', # 71 tracts\n    'Alexandria city', # 48 tracts\n    'Fairfax city', # 5 tracts\n    'Falls Church city', # 3 tracts\n]\ndmv_df = dmv_df[dmv_df['COUNTY'].isin(counties)].copy()\n\nAnd now, since we‚Äôre about to merge this data with the shapefiles for Maryland, DC, and Virginia, which have a GEOID field of type string, we‚Äôll need to create a string version of the TL_GEO_ID variable from IPUMS, for merging:\n\n# String version for merging\ndmv_df['TL_GEO_ID_str'] = dmv_df['TL_GEO_ID'].apply(str)"
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html#tiger-shapefiles-for-dc-maryland-and-virginia",
    "href": "posts/2023-11-27-dc-segregation/index.html#tiger-shapefiles-for-dc-maryland-and-virginia",
    "title": "Visualizing Segregation in DC",
    "section": "TIGER Shapefiles for DC, Maryland, and Virginia",
    "text": "TIGER Shapefiles for DC, Maryland, and Virginia\nNext we‚Äôll load the TIGER shapefiles provided by the Census website, for DC (FIPS code 11), Maryland (FIPS code 24), and Virginia (FIPS code 51).\nHere we use the amazing GeoPandas library, which essentially lets us keep using Pandas as we‚Äôve been using it, but also maintains a GIS representation of the data ‚Äúunder the hood‚Äù, so that when we‚Äôre ready to plot our data we can plug the GeoDataFrame object into (for example) Plotly or any other data visualization library that supports mapping!\n\n# Uncomment the following to install geopandas, if it is\n# not already installed in your environment!\n#!pip install geopandas\n\n\nimport geopandas as gpd\n# Shapefiles\ndc_shape_df = gpd.read_file(\"assets/tl_2021_11_tract/tl_2021_11_tract.shp\")\nmd_shape_df = gpd.read_file(\"assets/tl_2021_24_tract/tl_2021_24_tract.shp\")\nva_shape_df = gpd.read_file(\"assets/tl_2021_51_tract/tl_2021_51_tract.shp\")\ndmv_shape_df = pd.concat([dc_shape_df,md_shape_df,va_shape_df], ignore_index=True)\n\nNow, since our original dmv_df and the GeoPandas-managed dmv_shape_df both have GEO_ID variables (with slightly different names), we can merge them into a single DataFrame and then tell GeoPandas to track all of this information!\n\ngeo_df_pd = pd.merge(dmv_df, dmv_shape_df, left_on='TL_GEO_ID_str', right_on='GEOID', how='left')\ngeo_df = gpd.GeoDataFrame(geo_df_pd)\ngeo_df.set_index('GEOID', inplace=True)"
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html#visualizing-with-plotly",
    "href": "posts/2023-11-27-dc-segregation/index.html#visualizing-with-plotly",
    "title": "Visualizing Segregation in DC",
    "section": "Visualizing with Plotly",
    "text": "Visualizing with Plotly\nAnd now, finally, we can make use of Plotly‚Äôs Cloropethmapbox object to create a Cloropeth map of the different income levels:\n\n# Uncomment the following to install Plotly, if it is not already\n# installed on your machine!\n#!pip install plotly\n\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nmedian_income_var = \"AOQIE001\"\n# Capitol Building\n#capitol_lat = 38.889805\n#capitol_lon = -77.009056\n# White House\n#center_lat = 38.8977\n#center_lon = -77.0365\n# Scott Statue\ncenter_lat = 38.907278946266466\ncenter_lon = -77.03651807332851\n\nincome_fig = px.choropleth_mapbox(\n    geo_df,\n    geojson=geo_df.geometry,\n    locations=geo_df.index,\n    #z=geo_df[median_income_var],\n    color=median_income_var,\n    #autocolorscale=True,\n    opacity=0.7,\n    mapbox_style='carto-positron',\n    zoom = 10.4,\n    center = {\n        \"lat\": center_lat,\n        \"lon\": center_lon,\n    },\n    # width=800,\n    # height=800\n)\nincome_fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nincome_fig.show()\n\n                                                \n\n\nNotice anything? ‚Ä¶I feel like the raw median income distribution pretty much tells the whole story, but if we want to fully recreate the Pew maps, we could collapse these income levels down into (low, medium, high) using the methodology from the report‚Äôs appendix to produce a map of categorical income levels. For 2021, the most recent year for which IPUMS had the 5-year ACS data, the median income for the DC metro area was $110,355 (for comparison, the national median household income was $70,784), so that (letting \\(M\\) represent this metro-area median)\n\nThe cutoff for low-income (using Pew‚Äôs methodology) is \\(\\frac{2}{3}\\cdot M\\) = $73,570\nThe cutoff for high-income (again using Pew‚Äôs methodology) is \\(2M\\) = $220,710\n\n\nmedian_income_var = \"AOQIE001\"\n# Capitol Building\n#capitol_lat = 38.889805\n#capitol_lon = -77.009056\n# White House\ncenter_lat = 38.8977\ncenter_lon = -77.0365\n\n# Here we'll drop NA, since Plotly doesn't handle\n# NA values as well as NaN values\ngeo_df_nona = geo_df[~pd.isna(geo_df[median_income_var])].copy()\n# Cutpoints\n#natl_median = 70000\nmetro_median = 110355\nlow_cutoff = (2/3) * metro_median\nhigh_cutoff = 2 * metro_median\ndef get_income_level(income):\n    # If NA, we want to keep its category as NA\n    if pd.isna(income):\n        return pd.NA\n    if income &lt; low_cutoff:\n        return \"Low\"\n    if income &gt; high_cutoff:\n        return \"High\"\n    return \"Medium\"\ngeo_df_nona['income_level'] = geo_df_nona[median_income_var].apply(get_income_level)\nlevel_fig = px.choropleth_mapbox(geo_df_nona,\n  geojson=geo_df_nona.geometry,\n  color=\"income_level\",\n  locations=geo_df_nona.index,\n  #featureidkey=\"properties.district\",\n  center={\"lat\": center_lat, \"lon\": center_lon},\n  mapbox_style=\"carto-positron\",\n  hover_data=[median_income_var],\n  zoom=10,\n  color_discrete_map={\n    'High': 'green',\n    'Medium': 'lightgrey',\n    'Low': 'red'\n  }\n)\nlevel_fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nlevel_fig.show()\n\n                                                \n\n\nAnd voila! The pattern looks‚Ä¶ even more bleak in 2021 than it did in 2012 üòî"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "I Am Become Jeff",
    "section": "",
    "text": "A Meaningful Passage\n\n\nFor Someday-Parsing\n\n\n\nLife\n\n\nChasing the Owl of Minerva\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nThe NFL Has Become (Slightly) More Boring Over Time\n\n\n\n\n\n\nSports\n\n\nEntropy\n\n\nData Science\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Segregation in DC\n\n\n\n\n\n\nUrban Studies\n\n\nGIS\n\n\nSegregation\n\n\nInequality\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\nNo matching items"
  }
]