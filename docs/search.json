[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jeff Jacobs",
    "section": "",
    "text": "Hi, I‚Äôm Jeff Jacobs! I am an Assistant Teaching Professor of Data Science and Analytics at Georgetown University.\nI post things here when my brain becomes obsessed with something and needs to get to the bottom of it (since otherwise I annoy friends and family with these things for days on end üòú).\nIf you are one of my students, however, you may be looking for my DSAN Resources page, where I post things that are relevant to Georgetown DSAN students. Otherwise, you can find more on my academic homepage!"
  },
  {
    "objectID": "posts/2024-05-16-suburban-childhoods/index.html",
    "href": "posts/2024-05-16-suburban-childhoods/index.html",
    "title": "Starting at the Finish Line",
    "section": "",
    "text": "This is a strange post to try and write. There‚Äôs a passage in a Dave Eggers book I re-read recently (You Shall Know Our Velocity!), where the main character describes a council of incessant mental ‚Äúadvisors‚Äù who tap him on the shoulder throughout the day, reminding him of the most traumatic, embarrassing, and shameful aspects of his life. Well, I definitely have a similar team of mental consultants, who tap me on the shoulder every day just to remind me, ‚ÄúPsst, Hey, Jeff! Remember! Your opinions don‚Äôt matter! Nope, not even a little bit!‚Äù\nThe thing that makes this all difficult to write is the fact that, I still believe that‚Äôs completely true‚Äîin fact, in light of recent events with respect to friendships, interpersonal relationships, therapists, etc., I believe it now more than ever. What this means is, I can‚Äôt provide a narrative like ‚ÄúI used to believe this hyper-negative cynical thing about myself, but I managed to break out of it, and you can too!‚Äù\nSo then‚Ä¶ why write anything?\nBecause, long story short, I think there may be value in what I‚Äôll call a ‚Äúclimbing in through the window‚Äù approach to mental health: ways we may still be able to attain a modicum of emotional well-being even when we find the front door (the ‚Äústandard‚Äù approaches in popular self-help books, that seem to suffice for most people, maybe?) locked, and none of our keys are working.\nIt was another re-reading, of Beattie (1986), that pushed me to seek out a full-on outpatient mental health program a few weeks ago. I can‚Äôt explain exactly how I got from there to here, but maybe if you‚Äôve read that book or done this type of program it‚Äôll click: I had the thought that maybe, even if my opinion truly doesn‚Äôt matter to anyone else, it can still matter to me? It‚Äôs not so much that I believe that (because I don‚Äôt), but that I aspire to believe it. And‚Ä¶ while my subsequent instinct is to run to my journal, writing thoughts out here feels like a healthy step at the moment, as I‚Äôve reached a point where I can write dozens of pages in my journal every week, expressing how I feel in minute detail, yet I lock up the moment I try to express it outwardly, to anyone else.\nSo, here goes.\nI can‚Äôt speak for anyone else, but when I think about my childhood in DC ‚Üí adolescence in the DC suburbs there‚Äôs something extremely‚Ä¶ unsettling, dark, gut-wrenching, subtly horrifying about it. One thing I‚Äôve learned about myself recently is that I‚Äôm way too scared to describe anything about it directly, in my own words, but thankfully there are in fact tons of media representations of this‚Äîthe under-the-surface horror of suburbia. There‚Äôs a movie, Chumscrubbers, that really nails it I think, or there‚Äôs also the movie Palo Alto which covers it to some extent.\nFor me, though, there‚Äôs a spoken-word poem that I feel too possessive about to even name the artist1, that captures it way better than any of these movies do, and way better than I ever could. They‚Äôre talking about growing up in Smithfield, Rhode Island, and yet their description feels‚Ä¶ just as applicable to my subjective experience of Rockville, Maryland (if you end up finding a recording of the poem, you‚Äôll quickly understand why it has to be quoted in all-caps):\nAnd so, one of the reasons this clicks so hard for me is, how the extreme-ness of it shines a light on the discrepancy between suburban adolescence as something that is ‚Äúobjectively‚Äù nice, calm, controlled, peaceful, while at the same time feels subjectively miserable."
  },
  {
    "objectID": "posts/2024-05-16-suburban-childhoods/index.html#footnotes",
    "href": "posts/2024-05-16-suburban-childhoods/index.html#footnotes",
    "title": "Starting at the Finish Line",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy excuse: I support them as best as I can through bandcamp, kickstarter, etc., to make up for the non-exposure! Also, my ruse is very easily shattered by just googling the lyrics, lol‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html",
    "href": "posts/2023-11-27-dc-segregation/index.html",
    "title": "Visualizing Segregation in DC",
    "section": "",
    "text": "About ten years ago, Pew Research released an incredible set of maps visualizing how extreme segregation is in DC, race-wise as well as socioeconomic. A screenshot from this old set of visualizations shows what they used to look like:\nUnfortunately, all of these visualizations used MapBox, which seems to just totally not exist anymore (at least, these particular maps are long gone), so that when you try to view these visualizations on Pew‚Äôs website nowadays, you just get a blank page.\nSo, in this document, I recreate the above maps, using open-source libraries in Python to (hopefully) allow interactive visualization of this important information that will last longer than the previous versions in MapBox‚Äôs proprietary format!"
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html#data-overview",
    "href": "posts/2023-11-27-dc-segregation/index.html#data-overview",
    "title": "Visualizing Segregation in DC",
    "section": "Data Overview",
    "text": "Data Overview\nThe data behind these maps is somewhat hard to find, but in a strange way that is the opposite of most hard-to-find data cases: here there are so many different data sources for income across the ‚ÄúDC Metro Area‚Äù (the definition of this region, itself, being subject to different interpretations by different data sources), that I ran into the following tradeoff at the start:\n\nIf we want data for just the District of Columbia itself, we can obtain very easy-to-use data directly from the DC government‚Äôs data portal, which is ready for immediate use in the sense that we can plug it into a mapping app and see the data without any need to tweak any settings! Clicking that link, for example, will show a preview of the map directly within the GitHub page! While the GitHub preview won‚Äôt show the income data for each tract, this geojson.io link (with the URL just pointing to that GitHub page) will!\nSimilarly, if we want data for just Maryland or just Virginia, we could obtain fairly easy-to-use geoJSON files from these states‚Äô data portals\nBut, if we want data for the DC Metro Area, allowing apples-to-apples comparisons between (for example) census tracts within DC and in the Maryland suburbs, then we run into a bit of an issue since the relevant US Census data is far less ready-for-use in its raw form."
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html#ipums-data-median-income-by-census-tract",
    "href": "posts/2023-11-27-dc-segregation/index.html#ipums-data-median-income-by-census-tract",
    "title": "Visualizing Segregation in DC",
    "section": "IPUMS Data: Median Income by Census Tract",
    "text": "IPUMS Data: Median Income by Census Tract\nFirst we load the data, which contains median household income for all census tracts in the US:\n\nimport pandas as pd\nipums_df = pd.read_csv(\"assets/nhgis0001_ds254_20215_tract.csv\", encoding_errors='ignore')\nipums_df.head()\n\n\n\n\n\n\n\n\nGISJOIN\nYEAR\nSTUSAB\nREGIONA\nDIVISIONA\nSTATE\nSTATEA\nCOUNTY\nCOUNTYA\nCOUSUBA\n...\nPCI\nPUMAA\nGEO_ID\nBTTRA\nBTBGA\nTL_GEO_ID\nNAME_E\nAOQIE001\nNAME_M\nAOQIM001\n\n\n\n\n0\nG0100010020100\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020100\nNaN\nNaN\n1001020100\nCensus Tract 201, Autauga County, Alabama\n57399.0\nCensus Tract 201, Autauga County, Alabama\n10706.0\n\n\n1\nG0100010020200\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020200\nNaN\nNaN\n1001020200\nCensus Tract 202, Autauga County, Alabama\n52176.0\nCensus Tract 202, Autauga County, Alabama\n5849.0\n\n\n2\nG0100010020300\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020300\nNaN\nNaN\n1001020300\nCensus Tract 203, Autauga County, Alabama\n63704.0\nCensus Tract 203, Autauga County, Alabama\n11304.0\n\n\n3\nG0100010020400\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020400\nNaN\nNaN\n1001020400\nCensus Tract 204, Autauga County, Alabama\n70000.0\nCensus Tract 204, Autauga County, Alabama\n12155.0\n\n\n4\nG0100010020501\n2017-2021\nAL\nNaN\nNaN\nAlabama\n1\nAutauga County\n1\nNaN\n...\nNaN\nNaN\n1400000US01001020501\nNaN\nNaN\n1001020501\nCensus Tract 205.01, Autauga County, Alabama\n60917.0\nCensus Tract 205.01, Autauga County, Alabama\n29232.0\n\n\n\n\n5 rows √ó 45 columns\n\n\n\nWe can get a sense of how many Census Tracts there are across different states, before we restrict ourselves to just the DMV:\n\n# Here you can uncomment the following to install itables,\n# if it is not already installed in your environment!\n# We just use this to display nice HTML tables with pagination,\n# so it's optional and you don't need to worry if it\n# fails to install for whatever reason.\n#!pip install itables\n\n\nfrom itables import show\ntract_counts = ipums_df['STUSAB'].value_counts().to_frame().reset_index()\nshow(tract_counts)\n\n\n\n    \n      \n      index\n      STUSAB\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nBut now we can restrict our analysis to just DC, Maryland, and Virginia:\n\nstates_to_include = [\n    'District of Columbia',\n    'Maryland',\n    'Virginia'\n]\ndmv_df = ipums_df[ipums_df['STATE'].isin(states_to_include)].copy()\n\nAnd we can look at the 153 unique values that are listed in the ‚ÄúCounty‚Äù field for these states, where you‚Äôll see that this corresponds not only to ‚Äúcounties‚Äù in the standard colloquial sense but also to areas that have not been incorporated into any counties: places like Alexandria city:\n\ncounty_counts = dmv_df['COUNTY'].value_counts(dropna=False)\nshow(county_counts)\n\n\n\n    \n      \n      COUNTY\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nGoing through these unique values, I select the areas that seemed to be included in Pew‚Äôs ‚ÄúDC Metro Area‚Äù map:\n\ncounties = [\n    'Fairfax County', # 274 tracts\n    'Montgomery County', # 255 tracts\n    \"Prince George's County\", # 214 tracts\n    'District of Columbia', # 206 tracts\n    'Arlington County', # 71 tracts\n    'Alexandria city', # 48 tracts\n    'Fairfax city', # 5 tracts\n    'Falls Church city', # 3 tracts\n]\ndmv_df = dmv_df[dmv_df['COUNTY'].isin(counties)].copy()\n\nAnd now, since we‚Äôre about to merge this data with the shapefiles for Maryland, DC, and Virginia, which have a GEOID field of type string, we‚Äôll need to create a string version of the TL_GEO_ID variable from IPUMS, for merging:\n\n# String version for merging\ndmv_df['TL_GEO_ID_str'] = dmv_df['TL_GEO_ID'].apply(str)"
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html#tiger-shapefiles-for-dc-maryland-and-virginia",
    "href": "posts/2023-11-27-dc-segregation/index.html#tiger-shapefiles-for-dc-maryland-and-virginia",
    "title": "Visualizing Segregation in DC",
    "section": "TIGER Shapefiles for DC, Maryland, and Virginia",
    "text": "TIGER Shapefiles for DC, Maryland, and Virginia\nNext we‚Äôll load the TIGER shapefiles provided by the Census website, for DC (FIPS code 11), Maryland (FIPS code 24), and Virginia (FIPS code 51).\nHere we use the amazing GeoPandas library, which essentially lets us keep using Pandas as we‚Äôve been using it, but also maintains a GIS representation of the data ‚Äúunder the hood‚Äù, so that when we‚Äôre ready to plot our data we can plug the GeoDataFrame object into (for example) Plotly or any other data visualization library that supports mapping!\n\n# Uncomment the following to install geopandas, if it is\n# not already installed in your environment!\n#!pip install geopandas\n\n\nimport geopandas as gpd\n# Shapefiles\ndc_shape_df = gpd.read_file(\"assets/tl_2021_11_tract/tl_2021_11_tract.shp\")\nmd_shape_df = gpd.read_file(\"assets/tl_2021_24_tract/tl_2021_24_tract.shp\")\nva_shape_df = gpd.read_file(\"assets/tl_2021_51_tract/tl_2021_51_tract.shp\")\ndmv_shape_df = pd.concat([dc_shape_df,md_shape_df,va_shape_df], ignore_index=True)\n\nNow, since our original dmv_df and the GeoPandas-managed dmv_shape_df both have GEO_ID variables (with slightly different names), we can merge them into a single DataFrame and then tell GeoPandas to track all of this information!\n\ngeo_df_pd = pd.merge(dmv_df, dmv_shape_df, left_on='TL_GEO_ID_str', right_on='GEOID', how='left')\ngeo_df = gpd.GeoDataFrame(geo_df_pd)\ngeo_df.set_index('GEOID', inplace=True)"
  },
  {
    "objectID": "posts/2023-11-27-dc-segregation/index.html#visualizing-with-plotly",
    "href": "posts/2023-11-27-dc-segregation/index.html#visualizing-with-plotly",
    "title": "Visualizing Segregation in DC",
    "section": "Visualizing with Plotly",
    "text": "Visualizing with Plotly\nAnd now, finally, we can make use of Plotly‚Äôs Cloropethmapbox object to create a Cloropeth map of the different income levels:\n\n# Uncomment the following to install Plotly, if it is not already\n# installed on your machine!\n#!pip install plotly\n\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nmedian_income_var = \"AOQIE001\"\n# Capitol Building\n#capitol_lat = 38.889805\n#capitol_lon = -77.009056\n# White House\n#center_lat = 38.8977\n#center_lon = -77.0365\n# Scott Statue\ncenter_lat = 38.907278946266466\ncenter_lon = -77.03651807332851\n\nincome_fig = px.choropleth_mapbox(\n    geo_df,\n    geojson=geo_df.geometry,\n    locations=geo_df.index,\n    #z=geo_df[median_income_var],\n    color=median_income_var,\n    #autocolorscale=True,\n    opacity=0.7,\n    mapbox_style='carto-positron',\n    zoom = 10.4,\n    center = {\n        \"lat\": center_lat,\n        \"lon\": center_lon,\n    },\n    # width=800,\n    # height=800\n)\nincome_fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nincome_fig.show()\n\n                                                \n\n\nNotice anything? ‚Ä¶I feel like the raw median income distribution pretty much tells the whole story, but if we want to fully recreate the Pew maps, we could collapse these income levels down into (low, medium, high) using the methodology from the report‚Äôs appendix to produce a map of categorical income levels. For 2021, the most recent year for which IPUMS had the 5-year ACS data, the median income for the DC metro area was $110,355 (for comparison, the national median household income was $70,784), so that (letting \\(M\\) represent this metro-area median)\n\nThe cutoff for low-income (using Pew‚Äôs methodology) is \\(\\frac{2}{3}\\cdot M\\) = $73,570\nThe cutoff for high-income (again using Pew‚Äôs methodology) is \\(2M\\) = $220,710\n\n\nmedian_income_var = \"AOQIE001\"\n# Capitol Building\n#capitol_lat = 38.889805\n#capitol_lon = -77.009056\n# White House\ncenter_lat = 38.8977\ncenter_lon = -77.0365\n\n# Here we'll drop NA, since Plotly doesn't handle\n# NA values as well as NaN values\ngeo_df_nona = geo_df[~pd.isna(geo_df[median_income_var])].copy()\n# Cutpoints\n#natl_median = 70000\nmetro_median = 110355\nlow_cutoff = (2/3) * metro_median\nhigh_cutoff = 2 * metro_median\ndef get_income_level(income):\n    # If NA, we want to keep its category as NA\n    if pd.isna(income):\n        return pd.NA\n    if income &lt; low_cutoff:\n        return \"Low\"\n    if income &gt; high_cutoff:\n        return \"High\"\n    return \"Medium\"\ngeo_df_nona['income_level'] = geo_df_nona[median_income_var].apply(get_income_level)\nlevel_fig = px.choropleth_mapbox(geo_df_nona,\n  geojson=geo_df_nona.geometry,\n  color=\"income_level\",\n  locations=geo_df_nona.index,\n  #featureidkey=\"properties.district\",\n  center={\"lat\": center_lat, \"lon\": center_lon},\n  mapbox_style=\"carto-positron\",\n  hover_data=[median_income_var],\n  zoom=10,\n  color_discrete_map={\n    'High': 'green',\n    'Medium': 'lightgrey',\n    'Low': 'red'\n  }\n)\nlevel_fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nlevel_fig.show()\n\n                                                \n\n\nAnd voila! The pattern looks‚Ä¶ even more bleak in 2021 than it did in 2012 üòî"
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html",
    "href": "posts/2024-11-19_depression_pt2/index.html",
    "title": "What Does Depression Feel Like?",
    "section": "",
    "text": "Any time I even think about vocalizing (or even worse, writing down) something ‚Äúpersonal‚Äù, a shrill, persistent voice in my head begins screaming:\n\nShut up dude!\n\n\nNobody cares!\n\n\nYou‚Äôve got about 3 seconds to stop, before you overwhelm everyone in your immediate vicinity!\n\nAnd, even though some of the things talked about in Gibson (2015) didn‚Äôt exactly match up with my own experiences, one anecdote from it (the very first one in the book) did feel extremely familiar:\n\n‚ÄúIt was incredibly lonely, like I was utterly isolated. It was a fact of my existence. It just felt normal. In my family, everyone was separate from each other, and we were all emotionally isolated. We lived parallel lives, with no points of contact. In high school, I used to get this image of floating in the ocean with no one around me. That‚Äôs how it felt at home.‚Äù\nWhen I asked him more about the feeling of loneliness, he said, ‚ÄúIt was a sensation of emptiness and nothingness. I had no way of knowing that most people didn‚Äôt feel that way. That feeling was just daily life for me.‚Äù\n\nWhat one of my parents would call ‚Äúgetting brainwashed by my ‚Äòshrink‚Äô‚Äù over the past ~15 years, I think of as slowly coming to a realization: that, yes, the above is a pretty accurate description of how I felt when I was younger, and also captures my‚Ä¶ surprise? when therapists would point out some of the more damaging aspects of my childhood."
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html#i-had-no-way-of-knowing-that-most-people-didnt-feel-that-way",
    "href": "posts/2024-11-19_depression_pt2/index.html#i-had-no-way-of-knowing-that-most-people-didnt-feel-that-way",
    "title": "What Does Depression Feel Like?",
    "section": "",
    "text": "Any time I even think about vocalizing (or even worse, writing down) something ‚Äúpersonal‚Äù, a shrill, persistent voice in my head begins screaming:\n\nShut up dude!\n\n\nNobody cares!\n\n\nYou‚Äôve got about 3 seconds to stop, before you overwhelm everyone in your immediate vicinity!\n\nAnd, even though some of the things talked about in Gibson (2015) didn‚Äôt exactly match up with my own experiences, one anecdote from it (the very first one in the book) did feel extremely familiar:\n\n‚ÄúIt was incredibly lonely, like I was utterly isolated. It was a fact of my existence. It just felt normal. In my family, everyone was separate from each other, and we were all emotionally isolated. We lived parallel lives, with no points of contact. In high school, I used to get this image of floating in the ocean with no one around me. That‚Äôs how it felt at home.‚Äù\nWhen I asked him more about the feeling of loneliness, he said, ‚ÄúIt was a sensation of emptiness and nothingness. I had no way of knowing that most people didn‚Äôt feel that way. That feeling was just daily life for me.‚Äù\n\nWhat one of my parents would call ‚Äúgetting brainwashed by my ‚Äòshrink‚Äô‚Äù over the past ~15 years, I think of as slowly coming to a realization: that, yes, the above is a pretty accurate description of how I felt when I was younger, and also captures my‚Ä¶ surprise? when therapists would point out some of the more damaging aspects of my childhood."
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html#the-very-water-i-drink-the-very-air-i-breathe-would-feel-like-long-sharp-needles",
    "href": "posts/2024-11-19_depression_pt2/index.html#the-very-water-i-drink-the-very-air-i-breathe-would-feel-like-long-sharp-needles",
    "title": "What Does Depression Feel Like?",
    "section": "‚ÄúThe very water I drink, the very air I breathe, would feel like long, sharp needles‚Äù",
    "text": "‚ÄúThe very water I drink, the very air I breathe, would feel like long, sharp needles‚Äù\nThe corrollary to this realization‚Äîthe ‚Äúdoing something about it‚Äù part‚Äîhas been my journey to try and develop a language for expressing these previously-unnoticed feelings to people in my life (partners, friends, and others), rather than struggling in a wordless, panic-filled, isolated silence.\nTo stretch the language metaphor here a bit more, the part that keeps me up at night is how languages becomes more difficult to learn as you get older. To use an extreme example, but one that is often in the back of my mind: Genie, a woman who was prevented from acquiring language at all during her childhood, hasn‚Äôt been able to acquire fluency in adulthood either, despite being freed from her imposed isolation.\nThough I‚Äôm obviously not in the same scenario as Genie in an objective psychological/behavioral sense (I did acquire fluency in a language during childhood, for example, hence my emphasis on using ‚Äúfluency‚Äù here in a loose, metaphorical sense), subjectively I absolutely feel that I lack ‚Äúemotional fluency‚Äù, and that it has been immensely difficult trying to develop it in adulthood.\nGiven all this, in the same way that Genie is still able to use sounds, gestures, facial expressions, and so on to try and communicate, I‚Äôve found myself (for reasons I don‚Äôt entirely understand) holding tightly to the rare pieces of media‚Äîpoems, songs, movies‚Äîwhich feel like they are expressing something that I feel but that I‚Äôm unable to express in words.\nThe quote in the heading of this section, for example, comes from a Haruki Murakami novel (Sputnik Sweetheart, if I‚Äôm remembering correctly), which I read as a teenager and felt like an early example of ‚Äúholy shit, this is the thing I‚Äôve been trying, and failing, to express to people‚Äù:\n\nEvery now and then I would feel a violent stab of loneliness. The very water I drink, the very air I breathe, would feel like long, sharp needles. The pages of a book in my hands would take on the threatening metallic gleam of razor blades. I could feel the roots of loneliness creeping through me when the world was hushed at four o‚Äôclock in the morning.\n\nI have an absurdly distinct, clear, specific memory from early childhood‚Äîmaybe 8 or 9 years old‚Äîwhich matches this description: I can trace where I was down to the exact point on the globe. Walking along a fence separating the upper and lower fields at my elementary school when I was maybe 8 or 9, I looked out at the totally-empty fields and felt this exact violent stab of loneliness.\nSo, I think the main thing that‚Äôs driving me to write all this is just the desire to compile some of these pieces of media together in one place, so I can track my progress towards ‚Äúemotional fluency‚Äù. I want to compare how utterly stunned, floored, taken aback I was upon first encountering them‚Äîupon first realizing that these things were possible to communicate‚Äîwith my growing ability nowadays to communicate these types of feelings to others."
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html#those-lonely-giant-spaces-in-between-your-every-word",
    "href": "posts/2024-11-19_depression_pt2/index.html#those-lonely-giant-spaces-in-between-your-every-word",
    "title": "What Does Depression Feel Like?",
    "section": "‚ÄúThose lonely giant spaces in between your every word‚Äù",
    "text": "‚ÄúThose lonely giant spaces in between your every word‚Äù\nThe main books I remember having this effect, that I read at a way-way-too-young age, were probably‚Ä¶ Murakami, Hermann Hesse, Sartre‚Äôs Nausea, and probably some Dave Eggers books.\nBut, discovering Eyedea was almost certainly the first time I had encountered this feeling in a mode beyond ‚Äújust‚Äù language, i.e., words.\nDuring one of my many rough patches, though my first as an ‚Äúadult‚Äù (after high school), I discovered this song, which like many examples here I simply cannot listen to anymore because I will be too emotionally overwhelmed:\n\nAnd, even though I had made some simple songs and beats and whatnot earlier on in life (as any respectable awkward cringey suburban white hip hop fan does in their teens), this song definitely activated some new module in my brain, hence why I chose Lonely Giant Spaces as my‚Ä¶ artist/producer name."
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html#melancholia",
    "href": "posts/2024-11-19_depression_pt2/index.html#melancholia",
    "title": "What Does Depression Feel Like?",
    "section": "Melancholia",
    "text": "Melancholia\nThe next one that comes to mind would be the poem ‚ÄúKate‚Äù by B. Dolan, but I‚Äôve already covered that one here.\nBut if Eyedea blew my mind because of how the music mixed with the lyrics to produce something more expressive of the feeling than either would have generated individually, William Basinski blew my mind by showing me how completely instrumental music could capture just as much, if not more, of this type of feeling.\nI can listen to this piece (which turns out to be just the first track of the entire Melancholia album where every other track is equally gut-wrenching) for about‚Ä¶ 10 to 15 seconds before I become too overwhelmed and have to turn it off:"
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html#sch√∂nbergs-gaze",
    "href": "posts/2024-11-19_depression_pt2/index.html#sch√∂nbergs-gaze",
    "title": "What Does Depression Feel Like?",
    "section": "Sch√∂nberg‚Äôs Gaze",
    "text": "Sch√∂nberg‚Äôs Gaze\nAround the same time, as Basinski showed me how instrumental music on its own could capture these feelings, Sch√∂nberg (who is way more well-known for his atonal music, which kind of makes sense given this whole discussion) showed me how visual art could capture them. This portrait, his work just called Gaze, gave me goosebumps, butterflies, and everything else the first time I saw it, because (sadly but very very truly) it‚Äôs what I see when I look in a mirror, in a phenomenological sense:"
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html#i-look-into-the-mirror-all-im-seeing-is-a-skeleton",
    "href": "posts/2024-11-19_depression_pt2/index.html#i-look-into-the-mirror-all-im-seeing-is-a-skeleton",
    "title": "What Does Depression Feel Like?",
    "section": "‚ÄúI look into the mirror all I‚Äôm seeing is a skeleton‚Äù",
    "text": "‚ÄúI look into the mirror all I‚Äôm seeing is a skeleton‚Äù\nSpeaking of mirrors, this one was especially‚Ä¶ jarring? To discover since I had already been listening to nothing,nowhere for years before it came out, and yet it‚Äôs what moved him from ‚ÄúAmazing artist‚Äù to ‚ÄúUnquestionably-saved-my-life artist‚Äù in my mind:\n\nI can‚Äôt say too much about this particular era of my life and the impact this song had on it, because of reasons, but‚Ä¶ I can say I remember being somewhere near this point in the middle of Nevada in the middle of the night, totally lost in a literal and figurative sense, feeling like this song was someone suddenly handing me a flashlight in a dark, scary room."
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html#everything-was-moving-much-too-fast.-im-sorry.-i-think-things-might-be-starting-to-slow-down-now-im-not-sure",
    "href": "posts/2024-11-19_depression_pt2/index.html#everything-was-moving-much-too-fast.-im-sorry.-i-think-things-might-be-starting-to-slow-down-now-im-not-sure",
    "title": "What Does Depression Feel Like?",
    "section": "‚ÄúEverything was moving much too fast. I‚Äôm sorry. I think things might be starting to slow down now‚Ä¶ I‚Äôm not sure‚Äù",
    "text": "‚ÄúEverything was moving much too fast. I‚Äôm sorry. I think things might be starting to slow down now‚Ä¶ I‚Äôm not sure‚Äù\nIn my current‚Ä¶ I don‚Äôt know what to call it besides ‚Äúcurrent seemingly-inescapable rut‚Äù, part of what has been so rough at first glance is that I haven‚Äôt exactly found one specific piece of media that captures the feeling. But, when I write that out, I realize that this is probably a sign of progress‚Äîmy need to ‚Äúhide‚Äù behind these pieces of media is hopefully diminishing as my ability to express them in my own words improves. But, as a final point of comparison for my future self, these are two special songs that have brought comfort to me in the days since May 7th, 2024:"
  },
  {
    "objectID": "posts/2024-11-19_depression_pt2/index.html#references",
    "href": "posts/2024-11-19_depression_pt2/index.html#references",
    "title": "What Does Depression Feel Like?",
    "section": "References",
    "text": "References\n\n\nGibson, Lindsay C. 2015. Adult Children of Emotionally Immature Parents: How to Heal from Distant, Rejecting, or Self-Involved Parents. New Harbinger Publications."
  },
  {
    "objectID": "posts/2024-07-02-knieval/index.html",
    "href": "posts/2024-07-02-knieval/index.html",
    "title": "The Skycycle Blues",
    "section": "",
    "text": "In another installation of, Jeff posting things on here in order to avoid ranting at friends over WhatsApp‚Ä¶ I‚Äôm geeking over how incredible B. Dolan‚Äôs poems are. In my last post I already shared one of them, about traumatic suburban childhoods, but this one is about the motorcycle stunt jumper Evel Knieval.\nI umm‚Ä¶ would love to be able to write things like this someday. I‚Äôm guessing it‚Äôs related to the mathy/pattern-recognition-y analytic-ness of my brain, but something about these poems just really hits different‚Äîto me it feels like, every tiny little detail here is perfectly placed, so as to coalesce into an absolutely brilliant whole. It‚Äôs like Seurat but, even the little dots are themselves mini works of art? Anyways, these are just the specific parts of it that give me maximal goosebumps.\n\nSomewhere, in between heaven and the landing ramp, Lies the sacred mathematics of chance, the calculated risk.\nGasoline. Throttle. Thumbs up. Let her fly! And tear into the fabric of an instant Where you can live an entire lifetime In the star-dusted, flash-bulb infinity Of an impossible launch into space, That climbs to the top of its arc And beats the sky back another inch.\nOnly to crumble, and collapse. Only to fall, and return to the earth With no illusions of immortality, And pay the cost of dreaming.\nLike your skin stretched out in ribbons Along a hundred yards of hot tar. Like those ghostly, ruined bones On the X-ray photo. Like the steel plates, The pins and the screws they jam into you, Until you‚Äôve got more in common with your bike Than you do with any human being.\nBy 1976, Evel Knievel‚Äôs body is a monument in ruins. The scorched remains of the war He waged against his own flesh. Kids come home crippled from Vietnam Writing him letters saying, ‚ÄúThank you, sir. I figure, if you can get back up and go on, Then I can too.‚Äù\n\n(And, if you‚Äôre okay with spoilers‚Ä¶ the ending:)\n\nBut as he flies across the gaps Between his public appearances, Burning through cocaine like money, With women whose faces are made up To look like neon motel signs, He begins to feel the vacancy Of a million lights shining on his skin.\nYou‚Äôre Evel Knieval, the twinkle in America‚Äôs eye.\nBut every day you age another year. And you watch, as if in a dream, As you fail every single person in your life, To pay the cost of dreaming.\nThe botched attempts, the bankruptcies, The divorce, the loss of your family. All the bad blood in your veins, From hepatitis, kidney failure. In a wheelchair, in old age, Living to discover yourself Shrinking to the size of a footnote. A novelty, a gag, an oddity. All of it part of the long, drawn out revenge Of the cowardly enemy, who was beside you all along.\nIn 2007, Robert Craig Knievel Tells the Hour of Power Christian Telecast About waking up and seeing the devil in his bedroom. Speaking carefully and slowly, The broken man explains to the congregation How he rose up in bed and yelled: ‚ÄúDevil! Devil!‚Äù ‚ÄúYou bastard, you! Get away from me!‚Äù ‚ÄúI cast you out of my life!‚Äù ‚ÄúI just got on my knees and prayed,‚Äù said Evel. ‚ÄúGot on my knees and prayed.‚Äù ‚ÄúI prayed that god would put his arms around me, And never ever ever let me go.‚Äù\nHe was not a good man, But he was a Great Man. And for that, he deserves Mercy, Death. Mercy."
  },
  {
    "objectID": "posts/2023-12-14-nfl-parity/index.html",
    "href": "posts/2023-12-14-nfl-parity/index.html",
    "title": "The NFL Has Become (Slightly) More Boring Over Time",
    "section": "",
    "text": "As a theme that will reappear across several of these posts, we‚Äôll see that when we try to translate the interpretive idea of ‚Äúinterestingness‚Äù into a measurable quantity, entropy will be precisely the tool we‚Äôll want to use! As a reminder, information entropy is just a measure of how predictable the outcome of a stochastic process is:\n\n\n\n\n\nIn the above figure, for example, each point in the plot represents a distribution, which we can think of like the contents of a ‚Äúbag‚Äù that we are going to reach into and pull an object out of:\n\nThe ‚Äúbag‚Äù in the middle with equally many plus signs and minus signs has the highest entropy over all possible ‚Äúbags‚Äù, since we cannot predict better than 50/50 what we will pull out when we reach into the bag.\nThe bag all the way on the left, on the other hand, has the lowest possible entropy, since when we reach into this bag we know with 100% certainty that we will pull out a minus sign (and similarly for the bag all the way on the right, where we know with 100% certainty that we will pull out a plus sign).\n\nIn one of my favorite sports analyses of all time, for example, Jon Bois demonstrates the usefulness of entropy by going through every NFL team‚Äôs historcal record, whereby we can see that consistent teams (whether consistently good or consistently bad) are exactly those teams with the lowest entropy, while the most volatile teams, manically oscillating between dominant and pathetic seasons, have the highest entropy:\n\nIn this analysis I‚Äôm interested in a similar phenomenon, but from the perspective of someone like me: my home team has been so consistently, spectacularly bad for my entire life that I‚Äôve had to just enjoy watching the NFL as a whole, rather than following that one specific team. Because of this, to me, ‚Äúexciting‚Äù seasons are ones in which any team could potentially beat any other team on a given day, whereas ‚Äúboring‚Äù seasons are those in which the usual dynasties (in the 90s: Packers, Broncos; in the 2000s: Patriots, Colts) dominate all others.\n\nSo, to quell my curiosity, I used the same dataset but ranked each season in terms of unpredictability. That is, in terms of how well we can predict the winner by knowing which team has a better record. This way of looking at it is captured perfectly by the phrase often used by announcers after astonishing upsets: ‚ÄúThat‚Äôs why they play the game!‚Äù"
  },
  {
    "objectID": "posts/2023-12-14-nfl-parity/index.html#what-makes-an-nfl-season-interesting",
    "href": "posts/2023-12-14-nfl-parity/index.html#what-makes-an-nfl-season-interesting",
    "title": "The NFL Has Become (Slightly) More Boring Over Time",
    "section": "",
    "text": "As a theme that will reappear across several of these posts, we‚Äôll see that when we try to translate the interpretive idea of ‚Äúinterestingness‚Äù into a measurable quantity, entropy will be precisely the tool we‚Äôll want to use! As a reminder, information entropy is just a measure of how predictable the outcome of a stochastic process is:\n\n\n\n\n\nIn the above figure, for example, each point in the plot represents a distribution, which we can think of like the contents of a ‚Äúbag‚Äù that we are going to reach into and pull an object out of:\n\nThe ‚Äúbag‚Äù in the middle with equally many plus signs and minus signs has the highest entropy over all possible ‚Äúbags‚Äù, since we cannot predict better than 50/50 what we will pull out when we reach into the bag.\nThe bag all the way on the left, on the other hand, has the lowest possible entropy, since when we reach into this bag we know with 100% certainty that we will pull out a minus sign (and similarly for the bag all the way on the right, where we know with 100% certainty that we will pull out a plus sign).\n\nIn one of my favorite sports analyses of all time, for example, Jon Bois demonstrates the usefulness of entropy by going through every NFL team‚Äôs historcal record, whereby we can see that consistent teams (whether consistently good or consistently bad) are exactly those teams with the lowest entropy, while the most volatile teams, manically oscillating between dominant and pathetic seasons, have the highest entropy:\n\nIn this analysis I‚Äôm interested in a similar phenomenon, but from the perspective of someone like me: my home team has been so consistently, spectacularly bad for my entire life that I‚Äôve had to just enjoy watching the NFL as a whole, rather than following that one specific team. Because of this, to me, ‚Äúexciting‚Äù seasons are ones in which any team could potentially beat any other team on a given day, whereas ‚Äúboring‚Äù seasons are those in which the usual dynasties (in the 90s: Packers, Broncos; in the 2000s: Patriots, Colts) dominate all others.\n\nSo, to quell my curiosity, I used the same dataset but ranked each season in terms of unpredictability. That is, in terms of how well we can predict the winner by knowing which team has a better record. This way of looking at it is captured perfectly by the phrase often used by announcers after astonishing upsets: ‚ÄúThat‚Äôs why they play the game!‚Äù"
  },
  {
    "objectID": "posts/2023-12-14-nfl-parity/index.html#data-analysis",
    "href": "posts/2023-12-14-nfl-parity/index.html#data-analysis",
    "title": "The NFL Has Become (Slightly) More Boring Over Time",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\nCode\nimport pandas as pd\nimport numpy as np\ncombined_df = pd.read_csv(\"assets/nfl_combined.csv\")\ncombined_df.head()\n\n\n\n\n\n\n\n\n\ngame_id\nseason\ngame_type\nweek\ngameday\nweekday\ngametime\naway_team\naway_score\nhome_team\n...\nWinner/tie\nat\nLoser/tie\nUnnamed: 7\nPtsW\nPtsL\nYdsW\nTOW\nYdsL\nTOL\n\n\n\n\n0\n1999_01_MIN_ATL\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nMIN\n17.0\nATL\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1999_01_KC_CHI\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nKC\n17.0\nCHI\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1999_01_PIT_CLE\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nPIT\n43.0\nCLE\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1999_01_OAK_GB\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nOAK\n24.0\nGB\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1999_01_BUF_IND\n1999\nREG\n1.0\n1999-09-12\nSunday\nNaN\nBUF\n14.0\nIND\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 61 columns\n\n\n\nThen, because my brain is forever stuck in an object-oriented mode (and because I‚Äôm teaching Data Structures in Python next semester!), I decided to keep track of each team‚Äôs record throughout each season using custom Season and SeasonTeam objects:\n\n\nCode\n# Get the range of seasons from the df\nfirst_year_full = combined_df['season'].min()\nlast_year_full = combined_df['season'].max()\nprint(first_year_full, last_year_full)\nyear_range_full = list(range(first_year_full, last_year_full + 1))\nclass Season:\n    def __init__(self, year, team_id_list):\n        self.year = year\n        # Keys will be {year}_{team}\n        self.teams = {team_id: SeasonTeam(team_id, self.year) for team_id in team_id_list}\n        \n    def __str__(self):\n        all_teams = self.get_team_list()\n        first_team = all_teams[0]\n        last_team = all_teams[-1]\n        return f\"Season[year={self.get_year()},{self.get_num_teams()} teams: [{first_team}, ..., {last_team}]]\"\n    \n    def __repr__(self):\n        return self.__str__()\n        \n    def add_team(self, team_id):\n        self.teams[team_id] = SeasonTeam(team_id, self.year)\n        \n    def get_num_teams(self):\n        return len(self.get_team_list())\n    \n    def get_team(self, team_id):\n        return self.teams[team_id]\n    \n    def get_team_list(self):\n        return list(self.teams.keys())\n    \n    def get_team_record(self, team_id):\n        return self.get_team(team_id).get_record()\n    \n    def get_year(self):\n        return self.year\n    \n    def record_result(self, team_id, result):\n        self.get_team(team_id).record_result(result)\n\nclass SeasonTeam:\n    def __init__(self, team, year):\n        self.team = team\n        self.year = year\n        # (w,l,t), first week starts at (0,0,0)\n        self.record = np.array([0,0,0])\n    \n    def get_record(self):\n        return self.record\n    \n    def record_result(self, result):\n        new_record = self.get_record() + result\n        self.set_record(new_record)\n        \n    def set_record(self, new_record):\n        self.record = new_record\n\n\n1987 2021\n\n\nSo that now we can use a dictionary of Season objects to keep track of season-by-season data:\n\n\nCode\nunique_home = set(combined_df['away_team'].unique())\nunique_away = set(combined_df['home_team'].unique())\nunique_teams_set = unique_home.union(unique_away)\nteam_ids = sorted(list(unique_teams_set))\nprint(team_ids)\nprint(year_range_full)\nseasons = {cur_year: Season(cur_year, team_ids) for cur_year in year_range_full}\nprint(seasons[1999])\nprint(seasons.keys())\n\n\n['ARI', 'ATL', 'BAL', 'BUF', 'CAR', 'CHI', 'CIN', 'CLE', 'DAL', 'DEN', 'DET', 'GB', 'HOU', 'IND', 'JAX', 'KC', 'LA', 'LAC', 'LAR', 'LV', 'MIA', 'MIN', 'NE', 'NO', 'NYG', 'NYJ', 'OAK', 'PHI', 'PIT', 'SD', 'SEA', 'SF', 'STL', 'TB', 'TEN', 'WAS']\n[1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\nSeason[year=1999,36 teams: [ARI, ..., WAS]]\ndict_keys([1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021])\n\n\nAnd after defining some helper functions:\n\n\nCode\n# Ties count as 0.5 win and 0.5 loss\nwin_vec = np.array([1, 0, 0.5])\nloss_vec = np.array([0, 1, 0.5])\ndef compute_win_pct(record_vec):\n    if np.sum(record_vec) == 0:\n        # No games played yet, win pct considered 0\n        return 0\n    total_wins = np.dot(record_vec, win_vec)\n    total_losses = np.dot(record_vec, loss_vec)\n    win_pct = total_wins / (total_wins + total_losses)\n    return win_pct\n\ndef compare_records(record_a, record_b):\n    pct_a = compute_win_pct(record_a)\n    pct_b = compute_win_pct(record_b)\n    if pct_a &gt; pct_b:\n        return 1\n    if pct_b &gt; pct_a:\n        return -1\n    return 0\n\n\nI processed each game in a giant, completely-inefficient loop, which could definitely be done in a more efficient data-sciency way!\n\n\nCode\nall_result_data = []\nfor row_index, row in combined_df.iterrows():\n    cur_game_id = row['game_id']\n    cur_season = row['season']\n    season_obj = seasons[cur_season]\n    cur_week = row['week']\n    cur_away = row['away_team']\n    cur_home = row['home_team']\n    cur_result = row['result']\n    #print(cur_away, cur_home, cur_result)\n    away_pre_record = season_obj.get_team_record(cur_away)\n    home_pre_record = season_obj.get_team_record(cur_home)\n    away_better = compare_records(away_pre_record, home_pre_record)\n    if away_better &gt; 0:\n        better_team = cur_away\n    elif away_better &lt; 0:\n        better_team = cur_home\n    else:\n        better_team = \"none\"\n    #print(cur_away, away_pre_record, cur_home, home_pre_record, away_better)\n    if cur_result &lt; 0:\n        # Away team won\n        winning_team = cur_away\n        away_result = np.array([1,0,0])\n        home_result = np.array([0,1,0])\n    elif cur_result &gt; 0:\n        # Home team won\n        winning_team = cur_home\n        home_result = np.array([1,0,0])\n        away_result = np.array([0,1,0])\n    else:\n        # Tie\n        winning_team = \"none\"\n        away_result = np.array([0,0,1])\n        home_result = np.array([0,0,1])\n    season_obj.record_result(cur_away, away_result)\n    season_obj.record_result(cur_home, home_result)\n    away_post_record = season_obj.get_team_record(cur_away)\n    home_post_record = season_obj.get_team_record(cur_home)\n    #print(cur_away, away_post_record, cur_home, home_post_record)\n    # Now we can create the results data\n    result_data = {\n        'game_id': cur_game_id,\n        'away_pre': away_pre_record,\n        'home_pre': home_pre_record,\n        'better_team': better_team,\n        'winning_team': winning_team,\n        'better_won': (better_team != \"none\") and (better_team == winning_team),\n        'away_result': away_result,\n        'home_result': home_result,\n        'away_post': away_post_record,\n        'home_post': home_post_record\n    }\n    all_result_data.append(result_data)\n\n\nThe all_result_data list can now immediately be converted into a Pandas DataFrame:\n\n\nCode\nresult_df = pd.DataFrame(all_result_data)\nresult_df.head()\n\n\n\n\n\n\n\n\n\ngame_id\naway_pre\nhome_pre\nbetter_team\nwinning_team\nbetter_won\naway_result\nhome_result\naway_post\nhome_post\n\n\n\n\n0\n1999_01_MIN_ATL\n[0, 0, 0]\n[0, 0, 0]\nnone\nMIN\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n\n\n1\n1999_01_KC_CHI\n[0, 0, 0]\n[0, 0, 0]\nnone\nCHI\nFalse\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n\n\n2\n1999_01_PIT_CLE\n[0, 0, 0]\n[0, 0, 0]\nnone\nPIT\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n\n\n3\n1999_01_OAK_GB\n[0, 0, 0]\n[0, 0, 0]\nnone\nGB\nFalse\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n\n\n4\n1999_01_BUF_IND\n[0, 0, 0]\n[0, 0, 0]\nnone\nIND\nFalse\n[0, 1, 0]\n[1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\n\n\n\n\n\n\n\nBut this reveals an important consideration, which is that we should specifically focus on only those games where there was a team with an unambiguously-better record:\n\n\nCode\nresult_comp_df = result_df[result_df['better_team'] != \"none\"].copy()\nresult_comp_df.head()\n\n\n\n\n\n\n\n\n\ngame_id\naway_pre\nhome_pre\nbetter_team\nwinning_team\nbetter_won\naway_result\nhome_result\naway_post\nhome_post\n\n\n\n\n15\n1999_02_PIT_BAL\n[1, 0, 0]\n[0, 1, 0]\nPIT\nPIT\nTrue\n[1, 0, 0]\n[0, 1, 0]\n[2, 0, 0]\n[0, 2, 0]\n\n\n17\n1999_02_JAX_CAR\n[1, 0, 0]\n[0, 1, 0]\nJAX\nJAX\nTrue\n[1, 0, 0]\n[0, 1, 0]\n[2, 0, 0]\n[0, 2, 0]\n\n\n18\n1999_02_SEA_CHI\n[0, 1, 0]\n[1, 0, 0]\nCHI\nSEA\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 1, 0]\n[1, 1, 0]\n\n\n23\n1999_02_OAK_MIN\n[0, 1, 0]\n[1, 0, 0]\nMIN\nOAK\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 1, 0]\n[1, 1, 0]\n\n\n25\n1999_02_WAS_NYG\n[0, 1, 0]\n[1, 0, 0]\nNYG\nWAS\nFalse\n[1, 0, 0]\n[0, 1, 0]\n[1, 1, 0]\n[1, 1, 0]\n\n\n\n\n\n\n\nSo that now we can find the overall proportion of all games where the team with the better record indeed won:\n\n\nCode\nresult_comp_df['better_won'].mean()\n\n\n0.6188321787077619\n\n\nAnd then we can use Pandas‚Äô groupby() to compute this mean for each season:\n\n\nCode\nfrom itables import show\nresult_comp_df['season'] = result_comp_df['game_id'].apply(lambda x: int(x.split(\"_\")[0]))\nmean_by_season = result_comp_df.groupby('season')['better_won'].mean().reset_index()\nshow(mean_by_season)\n\n\n\n\n    \n      \n      season\n      better_won\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.0.1 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/2023-12-14-nfl-parity/index.html#plotting-results",
    "href": "posts/2023-12-14-nfl-parity/index.html#plotting-results",
    "title": "The NFL Has Become (Slightly) More Boring Over Time",
    "section": "Plotting Results",
    "text": "Plotting Results\nFirst, plotting these means as a line graph gives some insight, but is a bit messy due to the season-to-season volatility:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#plt.figure(figsize=(11,8))\nseason_plot = sns.lineplot(x='season', y='better_won', data=mean_by_season, marker='o')\n#plt.xticks(rotation=45, ha='right')\nplt.title(\"Predictability of NFL Games, 1987-2021\")\nplt.xlabel(\"Season\")\nplt.ylabel(\"Pr(Win | Better Record)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can start to see a general trend, though, if we plot a line of best fit, with the important caveats that this is to indicate a general trend, not that we actually think the underlying data-generating process is linear!\n\n\nCode\nseason_regplot = sns.regplot(x='season', y='better_won', data=mean_by_season, ci=89) #, lowess=True)\nseason_regplot.axhline(mean_by_season['better_won'].mean(), linestyle=\"dashed\")\nplt.title(\"Predictability of NFL Games, 1987-2021\")\nplt.xlabel(\"Season\")\nplt.ylabel(\"Pr(Win | Better Record)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd there you have it: the NFL has gotten slightly more boring over time, as measured by the ability to predict game outcomes from the records of the teams going into the game‚Ä¶"
  },
  {
    "objectID": "posts/2024-05-06-a-meaningful-passage/index.html",
    "href": "posts/2024-05-06-a-meaningful-passage/index.html",
    "title": "A Meaningful Passage",
    "section": "",
    "text": "(My sloppy attempt at translation! And embedding fonts!)\n\nKamaswami told him about his business, he showed him wares and warehouses, showed him accounts. Siddhartha learned. He heard much and said little. He treated it all as a game, whose rules he strove to learn precisely, but whose content did not touch his heart.\nIn matters of love he was childish. He tended to pursue blindly, endlessly, insatiably. She taught him to balance taking with giving, and that every gesture, every caress, every touch, every glance, every aspect of a connection has its secret to patiently unlock. She taught him that in each celebration of love the lovers should admire each other, without being conquered or having conquered, so that neither is glutted, neither has the feeling of having misused or having been misused. He spent wonderful hours with the clever and beautiful artist, became her pupil, her lover, her friend. Here, with Kamala, lay the value and purpose of his current life, and not with Kamaswami‚Äôs business.\n\n(Hesse 1922)\n\n\n\n[Update, May 16, 2024] From a few pages later‚Ä¶ for further translation practice/someday-parsing:\n\nThat evening she clenched him with painful urgency, tearing and scratching, as if trying to squeeze the last remaining drops of sweetness from this ephemeral, exhausted pleasure. Never had it been so unsettlingly clear to Siddhartha how closely lust is related to death.\nFatigue was written on Kamala‚Äôs face, the fatigue of fighting a long, losing battle. He parted from her, disgust congealing in the pit of his stomach like tepid, spoiled wine. He could no longer bear the evening atmosphere: the too-sweet, bleak music, the too-soft smiles, the sickly-sweet scents. But above all, he was disgusted at himself. He had, it seemed to him, been leading a worthless life. Worthless and senseless. Laying down beneath a mango tree, he felt death in his heart and horror in his breast. He sat and sensed his existence withering inside him, unraveling inside him, coming to an end. Siddhartha knew that the game was done, that he could play it no longer. A shudder ran through his body; inside him, he knew, something had died.\n\n\n\n\nHesse, Hermann. 1922. Siddhartha. New Directions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "I Am Become Jeff",
    "section": "",
    "text": "What Does Depression Feel Like?\n\n\nExpression Beyond Langue vs.¬†Parole\n\n\n\nLife\n\n\nDepression\n\n\n\n\n\n\n\n\n\nNov 19, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nThe Skycycle Blues\n\n\n\n\n\n\nArt\n\n\nPoetry\n\n\n\n\n\n\n\n\n\nJul 2, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nStarting at the Finish Line\n\n\nLives of Quiet Desperation in the US Suburbs\n\n\n\nLife\n\n\nFinding My Own Voice\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nA Meaningful Passage\n\n\nFor Someday-Parsing\n\n\n\nLife\n\n\nChasing the Owl of Minerva\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nThe NFL Has Become (Slightly) More Boring Over Time\n\n\n\n\n\n\nSports\n\n\nEntropy\n\n\nData Science\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Segregation in DC\n\n\n\n\n\n\nUrban Studies\n\n\nGIS\n\n\nSegregation\n\n\nInequality\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\nNo matching items"
  }
]